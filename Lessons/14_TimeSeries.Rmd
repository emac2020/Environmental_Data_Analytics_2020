---
title: "14: Time Series"
author: "Environmental Data Analytics | Kateri Salk"
date: "Spring 2020"
output: pdf_document
geometry: margin=2.54cm
editor_options: 
  chunk_output_type: console
---

## Objectives
1. Discuss the purpose and application of time series analysis for environmental data
2. Choose appropriate time series analyses for trend detection 
3. Address the influence of seasonality on time series analysis
4. Interpret and communicate results of time series analyses 

## Set up
```{r, message = FALSE}
getwd()
library(tidyverse)
library(lubridate)
#install.packages("trend")
library(trend)
#install.packages("zoo")
library(zoo)

# Set theme
mytheme <- theme_classic(base_size = 14) +
  theme(axis.text = element_text(color = "black"), 
        legend.position = "top")
theme_set(mytheme)

EnoDischarge <- read.csv("./Data/Processed/USGS_Site02085000_Flow_Processed.csv")
EnoDischarge$datetime <- as.Date(EnoDischarge$datetime, format = "%Y-%m-%d")

NCAir <- read.csv("./Data/Processed/EPAair_O3_PM25_NC1819_Processed.csv")
NCAir$Date <- as.Date(NCAir$Date, format = "%Y-%m-%d")
```


## Time Series Analysis

Time series are a special class of dataset, where a response variable (categorical or continuous) is tracked over time. The frequency of measurement and the timespan of the dataset can vary widely. At its most simple, a time series model includes an explanatory time component and a response variable. Mixed models can include additional explanatory variables (check out the `nlme` and `lme4` R packages). We will cover a few simple applications of time series analysis in these lessons, with references for how to take analyses further.

Could use mixed model framework for covariance (using time and something else to explain response variable) lme and lme4 both work

### Opportunities

Analysis of time series presents several opportunities. For environmental data, some of the most common questions we can answer with time series modeling are:

* Has there been an increasing or decreasing **trend** in the response variable over time?
* Can we **forecast** conditions in the future?

### Challenges

Time series datasets come with several caveats, which need to be addressed in order to effectively model the system. A few common challenges that arise (and can occur together within a single dataset) are: 

* **Autocorrelation**: Data points are not independent from one another (i.e., the measurement at a given time point is dependent on previous time point(s))
If sampling temp of yesterday to predict tomorrow, there is autocorrelation

* **Data gaps**: Data are not collected at regular intervals, necessitating *interpolation* between measurements. May need to adjust data so it's in regular intervals (interpolation)

* **Seasonality**: Cyclic patterns in variables occur at regular intervals, impeding clear interpretation of a monotonic (unidirectional) trend.
Temp outside is warmer and colder at certain times which may interupt our assumptions over time b/c not cyclical 

* **Heteroscedasticity**: The variance of the time series is not constant over time
Getting more variable or less variable over time

* **Covariance**: the covariance of the time series is not constant over time
Neither increasing nor decreasing, if not the case, may need to address a covariance issue

### Example dataset: Eno River Discharge

River discharge is measured daily at the Eno River gage station. Since we are working with one location measured over time, this will make a great example dataset for time series analysis. 

Let's look at what the dataset contains for mean daily discharge.

```{r}
ggplot(EnoDischarge, aes(x = datetime, y = discharge.mean)) +
  geom_line() +
  labs(x = "", y = expression("Discharge (ft"^3*"/s)"))

# daily mean discharge over time. 
# missing data, lots of variability, some outliers present, upward vs downward trend over years isn't obvious
# * indicates finished with subscript and can go back to regular text
```

Notice there are missing data from 1971 to 1985. Gaps this large are generally an issue for time series analysis, as we don't have a continuous record of data or a good way to characterize any variability that happened over those years. We will illustrate a few workarounds to address these issues. 

Let's start by removing the NAs and splitting the dataset into the early and late years. 

```{r}
EnoDischarge.complete <- EnoDischarge %>%
  drop_na(discharge.mean)
# drop_na instead of na.omit to just remove NAs from "discharge mean". working with just a single column 

# split into early and late to show 2 continuous periods of record to interpret separately 

EnoDischarge.early <- EnoDischarge.complete %>%
  filter(datetime < as.Date("1985-01-01"))
# if datetime is less than a certain time, still have to use as.Date

EnoDischarge.late <- EnoDischarge.complete %>%
  filter(datetime > as.Date("1985-01-01"))
```

## Decomposing a time series dataset

A given time series can be made up of several component series: 

1. A **seasonal** component, which repeats over a fixed known period (e.g., seasons of the year, months, days of the week, hour of the day)
Higher and lower discharge at certain times of year and could repeat over certain times of year
Could also be hour of the day for subO2 temps, etc. 
Working with seasonal and/or monthly in this class

2. A **trend** component, which quantifies the upward or downward progression over time. The trend component of a time series does not have to be monotonic.
Could be monotonic: increasing or decreasing
Could also be moving avg 

3. An **error** or **random** component, which makes up the remainder of the time series after other components have been accounted for. This component reflects the noise in the dataset. 
Any discharge that can't be explained from seasonality etc

4. (optional) A **cyclical** component, which repeats over periods greater than the seasonal component. A good example of this is El Ni√±o Southern Oscillation (ENSO) cycles, which occur over a period of 2-8 years.

We will decompose the EnoDischarge.late data frame for illustrative purposes today. It is possible to run time series analysis on detrended data by subtracting the trend component from the data. However, detrending must be done carefully, as many environmental data are bounded by zero but are not treated as such in a decomposition. If you plan to use decomposition to detrend your data, please consult time series analysis guides before proceeding.

We first need to turn the discharge data into a time series object in R. This is done using the `ts` function. Notice we can only specify one column of data and need to specify the period at which the data are sampled. The resulting time series object cannot be viewed like a regular data frame.

Note: time series objects must be equispaced. In our case, we have daily data with no NAs in the data frame, so we don't need to worry about this. We will cover how to address data that are not equispaced later in the lesson.

```{r}
EnoDischarge.late_ts <- ts(EnoDischarge.late[[8]], frequency = 365)

# Want it to be totally different object than Eno late b/c very different so must add _ts to the object
# can see that it has 12505 objects which is same amount of rows in data
# can't inspect it at this point so important to know what data looks like beforehand

# Double square brackets are to select specific column and frequency is look at data every 365 days 

#dataframes that turn into time series objects
# Must be equally spaced: need number for every day, measurement for every month etc.

#Could subtract trend from actual measurements to "detrend" the model
```

The `stl` function decomposes the time series object into its component parts. We must specify that the window for seasonal extraction is either "periodic" or a specific number of at least 7. The decomposition proceeds through a loess (locally estimated scatterplot smoothing) function (designating a moving avg across dataset and extracting points)

```{r}
?stl

# Generate the decomposition
EnoDischarge.late_Decomposed <- stl(EnoDischarge.late_ts, s.window = "periodic")

# has to use time series, can't be dataframe. have to specificy a window, can say periodic and it will look for it. can also do weekly (7), monthly (12), annual (365)

# Visualize the decomposed series. 
plot(EnoDischarge.late_Decomposed)

# Don't have monotonic trend. noting periods of time higher or lower, like timestep 20 
# specified 365 unit repeat and have daily units of data so it's showing how many repeats over years, not actually showing what it represents
# bars on left: relative difference in axes by relevant high of bar in comparison to the other bars
# Data: actual data measurements = seasonal + trend + remainder
# seasonal and trend make up small amount of data and so many random effects and outliers in data so most of data ends up as remainder which is why remainder looks similar to "data" but not actually the same numbers
# seasonal: eno doesn't necessarily have strong seasonal trend/signal. instead, have a lot of noise in seasonal (not surprising based on flow conditions) but each season looks exactly the same (b/c 35 years and 365 days in dataset). STL will always define seasonal component consistent over 35-years. Not necessary consistent. if strong, seasonal access would be larger and repeat would be more clear and not so noisy

# We can extract the components and turn them into data frames
EnoDischarge.late_Components <- as.data.frame(EnoDischarge.late_Decomposed$time.series[,1:3])
# go to timeseries list and grab all the rows (,) and columns (1:3)
# have to be sure dataframes are in order from earliest to latest
# can attach the numbers to actual dates and measurements below in columns "observe" and "date"
# can mutate on a whole column of data b/c they're the same length
# could also make "observed" by adding up seasonal, trend, and remainder but this works too

EnoDischarge.late_Components <- mutate(EnoDischarge.late_Components,
                      Observed = EnoDischarge.late$discharge.mean,     
                      Date = EnoDischarge.late$datetime)

# Visualize how the TREND maps onto the data
ggplot(EnoDischarge.late_Components) +
  geom_line(aes(y = Observed, x = Date),  size = 0.25) +
  geom_line(aes(y = trend, x = Date), color = "#c13d75ff", size = .8) +
  geom_hline(yintercept = 0, lty = 2) +
  ylab(expression("Discharge (ft"^3*"/s)"))

# specifiy aes in geom_lines b/c want different aesthetics for each
# second line is specifying trend component
# geom_hline is specifying 0 discharge, not necessary but helpful
# periods with high amt of discharge across several months is where we are getting high trends, identifying consistently high or low discharge

# Visualize how the SEASONAL cycle maps onto the data
ggplot(EnoDischarge.late_Components) +
  geom_line(aes(y = Observed, x = Date),  size = 0.25) +
  geom_line(aes(y = seasonal, x = Date), color = "#c13d75ff") +
  geom_hline(yintercept = 0, lty = 2) +
  ylab(expression("Discharge (ft"^3*"/s)"))

# plot seasonal instead of trend
# Seasonal component dips below 0 consistently every year, stl doesn't care if goes below axis
# have to interpret with caution cuz can have neg values if don't apply decomposition properly
# hline helps visualize the 0 line 

```

Note that the decomposition can yield negative values when we apply a seasonal adjustment or a trend adjustment to the data. The decomposition is not constrained by a lower bound of zero as discharge is in real life. Make sure to interpret with caution!


## Trend analysis

Two types of trends may be present in our time series dataset: **monotonic** or **step**. Monotonic trends are a gradual shift over time that is consistent in direction, for example in response to land use change. Step trends are a distinct shift at a given time point, for example in response to a policy being enacted. 
Decomposition is a moving avg for Eno river but could be step (lower value over certain amt of time that jumps higher i.e., could be policy change), monotonic responds to something more gradually (climate change, etc)

### Step trend analysis

Step trend analysis works well for upstream/downstream and before/after study design. We will not delve into each of these methods during class, but specific tests are listed below for future reference. 

Note: ALWAYS look into the assumptions of a given test to ensure it matches with your data and with your research question.

* **Change point detection**, e.g., `pettitt.test` (package: trend), `breakpoints` (package: strucchange), `chngpt.test` (package: chngpt), multiple tests in package: changepoint
* **t-test (paired or unpaired)**: looking at before and after
* **Kruskal-Wallis test**: non-parametric version of t-test : not fulilling assumptions of t-test
* **ANCOVA**, analysis of covariance: multiple correlations over time

#### Example: step trend analysis
Let's say we wanted to know whether discharge was higher in the early period or the late period. Perhaps there was a change in the methodology of streamflow measurement between the two periods that caused a differene in the magnitude of measured discharge?

```{r}
# want to see if discharge measurements were different over early and late periods
# can use t-test or Kruskal test
# assumptions for t-test: well-approximated over normal distibution, = variance across different data sets
# shapiro tests for normality (only works for sample size up to 5000)
# var.test tests for variance (no limit for sample size)

EnoDischarge.early.subsample <- sample_n(EnoDischarge.early, 5000)
EnoDischarge.late.subsample <- sample_n(EnoDischarge.late, 5000)

shapiro.test(EnoDischarge.early.subsample$discharge.mean)
shapiro.test(EnoDischarge.late.subsample$discharge.mean)

# not approximated by norm distribution b/c super low p-value
# null hypothesis is that they are well approximated by norm distribution so we reject the null b/c super low p-value and say that they aren't approx'd by normal distribution
# could have different w-value or p-value because its a random sub-sample (5000)

var.test(EnoDischarge.early$discharge.mean, EnoDischarge.late$discharge.mean)
# early vs late mean daily discharge
# specifies alt hypoth
# totally unequal variance across two datasets b/c very small p-value

#violated assumptions for t-test so now run a wilcox

wilcox.test(EnoDischarge.early$discharge.mean, EnoDischarge.late$discharge.mean)

# shows that two different periods are significantly different and that we are seeing sig differences in means discharge from two periods

summary(EnoDischarge.early$discharge.mean)
summary(EnoDischarge.late$discharge.mean)

# didn't have time as explanatory variable, instead used period to answer questions so not totally normal time series but helpful nonetheless
```

How might you interpret the results of this test, and how might you represent them graphically?

> mean daily measurements in early were sig higher than late. Have support from Wilcox that means and medians are sig different

> mean daily discharge in the Eno River from 1927-1971 was sig higher than mean daily discharge from 1985-2019 (Wilcoxon test, W = 1.78*10^8, p < 0.0001).

> Graphically: boxplot/density/violin to show different mean distribution. may need to data wrangle on back end to graph together b/c very different datasets

### Monotonic trend analysis

In general, detecting a monotonic trend requires a long sequence of data with few gaps. If we are working with monthly data, a time series of at least five years is recommended. Gaps can be accounted for, but a gap that makes up more than 1/3 of the sampling period is generally considered the threshold for considering a gap to be too long (a step trend analysis might be better in this situation). 

Any trends that are occurring consistently over time, increasing or decreasing.

Adjusting the data may be necessary to fulfill the assumptions of a trend test. These adjustments include **aggregation**, **subsampling**, and **interpolation**. What do each of these mean, and why might we want to use them?

> aggregation: bundling data and scaling up (take months and create yearly data). taking data points and making one data point from them. Could be useful when you have a lot of data. If you have missing data, could take a summary statistic to fill in gaps. Can't do this if missing many data points.

> subsampling: If have daily data, could take 1st of every month as monthly value. Used the least out of time series analyses.

> interpolation: Predicting what would be in data gaps by extrapolating from other complete data

Common interpolation methods: 

* **Piecewise constant**: also known as a "nearest neighbor" approach. Any missing data are assumed to be equal to the measurement made nearest to that date (could be earlier or later).
* **Linear**: could be thought of as a "connect the dots" approach. Any missing data are assumed to fall between the previous and next measurement, with a straight line drawn between the known points determining the values of the interpolated data on any given date.
* **Spline**: similar to a linear interpolation except that a quadratic function is used to interpolate rather than drawing a straight line. Overpredict upperbounds and underpredict lowerbounds.


#### Example: interpolation

The Eno River discharge data doesn't have any short periods of missing data, so interpolation would not be a good choice for that dataset. We will illustrate a linear interpolation of the NC Air quality dataset below. 

In this case, several sites have a lot of missing data, and several sites monitor most days with few missing data points. 
```{r}
NCOzone <-
ggplot(NCAir, aes(x = Date, y = Ozone)) +
  geom_point() +
  facet_wrap(vars(Site.Name))
print(NCOzone)

# can't interpolate across several months because will overestimate the low points from the complete datasets

NCPM2.5 <-
ggplot(NCAir, aes(x = Date, y = PM2.5)) +
  geom_point() +
  facet_wrap(vars(Site.Name))
print(NCPM2.5)

# See less seasonality with fewer seasonal gaps, but some stations haven't recorded data for 2019

summary(NCAir$Site.Name)
# gives how many samples/rows are present. if have complete set, should have 365 + 365 = 730 and see many do not have 730
# so want to find a site where theres some missing data (not a lot) that we can work with (i.e. Garinger High only missing 8 data points)

NCAir.Garinger <- NCAir %>%
  filter(Site.Name == "Garinger High School")

GaringerOzone <-
ggplot(NCAir.Garinger, aes(x = Date, y = Ozone)) +
  geom_point() 
print(GaringerOzone)

# some hidden days where we're missing data. Na.approx: if having missing day and have two days on either side of it, it will take the mean of those two days to fill in data for missing days

# na.approx function fills in NAs with a linear interpolation
# Spline interpolation can also be specified as an alternative
# Piecewise constant interpolation can be done with na.aggregate
NCAir.Garinger$Ozone <- na.approx(NCAir.Garinger$Ozone)
NCAir.Garinger$PM2.5 <- na.approx(NCAir.Garinger$PM2.5)

GaringerOzone.interpolated <-
ggplot(NCAir.Garinger, aes(x = Date, y = Ozone)) +
  geom_point() 
print(GaringerOzone.interpolated)

# Can see NA values in Ozone and PM2.5 in the dataframe now
# Plot: can see new points appearing in 2018 (shouldn't stand out b/c just taking the mean of days on either side). Not always an avg: if missing two data points, will take the one closest for day 2 (i.e.  day 1 data) and for day 3 data (i.e. day 4 data).
# will have to create a dataset for 365 days a year in hw 8. 
```

### Monotonic trend analysis, continued

Specific tests for monotonic trend analysis are listed below, with assumptions and tips: 

* **linear regression**: no seasonality, fits the assumptions of a parametric test. Function: `lm`
- Sub out time for x-axis with whatever x-axis we're using, equispace data, fits assumptions, and data is unidirectional
- can be good for time series analysis
* **Mann-Kendall**: no seasonality, non-parametric, no temporal autocorrelation, missing data allowed. Function: `mk.test` (package: trend)
- does takeaway assumption of having normally distributed
- no temp autocorrelation: points that are taken closer in time to each other are correlated. Further away you get in time, the less you are able to make predictions. 
- have to take samples far enough in time so they aren't temporally autocorrelated
* **modified Mann-Kendall**: no seasonality, non-parametric, accounts for temporal autocorrelation, missing data allowed. Function: `mmky` and `mmkh` (package: modifiedmk)
- can determine actual trend and what's influenced by how closely spaced the samples are taken to each other
* **Seasonal Mann-Kendall**: seasonality, non-parametric, no temporal autocorelation, identical distribution. Function: `smk.test` (package: trend)
- allows for seasonallity, but still dealing with non parametric and no temporal.
- have to have equispaced dataset (i.e. if taking monthly data have to have data for every single month, same with daily data)

The packages trend, Kendall, and modifiedmk also include other modifications to monotonic trend tests. Look into the documentation for these packages if you are applying a special case.

If covariates (another predictor variable) are included in the dataset, additional tests are recommended. A great resource for trend testing for water quality monitoring, which includes guidance on these cases, has been prepared by the Environmental Protection Agency: https://www.epa.gov/sites/production/files/2016-05/documents/tech_notes_6_dec2013_trend.pdf. This would likely be useful for other types of environmental data too. 

#### Example: monotonic trend analysis

Remember that we noticed in the decomposition that the Eno River discharge data has a seasonal cycle (despite high random variability). We might be interested in knowing how (if) discharge has changed over the course of measurement while incorporating the seasonal component. In this case, we will use a Seasonal Mann-Kendall test to figure out whether a monotonic trend exists. We will use the late dataset again.
- assumptions of no seasonality are not fulfilled so going with Mann-Kendall
- know with have temp auto, cuz daily discharge introduces temporal auto issues (if storm, know discharge will be stronger for those few days)

The Seasonal Mann-Kendall assumes no temporal autocorrelation, but we know that daily data is prone to temporal autocorrelation. In this case, we may want to collapse our data down into monthly data so that we can (1) reduce temporal autocorrelation and (2) break down the potential seasonal trend into more interpretable components. 
- aggregate data

We will calculate the mean monthly discharge for this dataset, rather than calculating the total monthly discharge or subsampling a given day in each month. Why did we make this decision?

```{r}
EnoDischarge.late.monthly <- EnoDischarge.late %>%
  mutate(Year = year(datetime), 
         Month = month(datetime)) %>%
  group_by(Year, Month) %>% # creating groupings for each year and each month
  summarise(Discharge = mean(discharge.mean)) # taking mean discharge for any given month. not median b/c median doesn't take in outliers. could also think about sum but the months don't have even # of days in them so it wont properly show, works better when aggregating by year
# 411 months of data over 35 years with 1 discharge measuements for each. columns will only be things we "group_by" and "summarise." missing the actual day. Can't graph monthly discharge over time without day
# call month/year combos "first of the month"
  
EnoDischarge.late.monthly$Date <- as.Date(paste(EnoDischarge.late.monthly$Year, 
                                                EnoDischarge.late.monthly$Month, 
                                                1, sep="-"), 
                                          format = "%Y-%m-%d")
#1. make new column called date on dataframe we have
# make new column as class of date
# paste all the data you have for year, month, and value of 1. then separate each of the numbers with a dash, and then want it classified as "date" so use the format = 
# don't use in time series analysis b/c not actually taken on the first of the month
# only the first 3 columns will be used in analysis (not the new date column. only included that for graphing purposes)

# Generate time series (smk.test needs ts, not data.frame)
EnoDischarge.late.monthly.ts <- ts(EnoDischarge.late.monthly$Discharge, frequency = 12, 
                        start = c(1985, 10, 1), end = c(2019, 12, 1))

# use ts function and call it something new b/c adjusting data 
# variable for which we're making the time series is the Discharge
# 12 months in a year, and frequency of analysi should come back around once every 12 months
# starting in 1985 and ending 2019. using first of the month
# time series won't contain infor on freq, start, and end, without specifiying it directly

# Run SMK test
EnoDischarge.late.trend <- smk.test(EnoDischarge.late.monthly.ts)

#smk.test = seasonal mann-kendall. always run test on time series object which is why we had to make that object
# want to make it an object so we can come back to it

# Inspect results
EnoDischarge.late.trend
# gives us: is there a monotonic trend over time? if there is, is it pos or neg? that's what this shows
# this tells us that there is not a signficant trend b/c p-value and z-score close to 0, meaning we can't discern a sig trend. slightly decreasing trend (-0.13967) but not sig from no trend at all
# - 1.96 or post 1.96 will indicate a positive or neg trend
# null: no sig trend over time
# alt.: there is a sig trend. true S is not equal to 0
summary(EnoDischarge.late.trend)
# is there a trend in actual seasons sepcified
# individual p-values from individual seasons. this is why have to tell it where it starts and ends. season 1 always jan and season 12 always december
# Not seeing significant trends across individual months b/c above .05 p-value. whenever have -s we also have - zscore. if add up difference from october in 1 year and october another year, that's your z-score.
# if had pos-pvalue for jan: overall, don't see significant seasonal trend, but do see some signficant pos trend in january

EnoDischarge.monthly <-
ggplot(EnoDischarge.late.monthly, aes(x = Date, y = Discharge)) +
  geom_point() +
  geom_line()
print(EnoDischarge.monthly)

# if plot monthly data, see a lot of variablity, but can't discern statistically whether there's a trend over time just by plotting. Expecting to see inc. drought and high flood conditions, but not a lot of evidence in data itself of eno river b/c so much variability
# seasonal cycle and trend were very small amount compared to random in that one breakdown of the 4 plots, so this goes along with that
```

What would we conclude based on these findings? 

> Overall, mean discharge in the Eno River does not have a significant seasonal trend (SMK, z= -0.13967, p > .05). We did not detect any trends across individual months within the dataset.
 
If a significant trend was present, we could compute a **Sen's Slope** to quantify that trend (`sens.slope` function in the trend package).



## Forecasting with Autoregressive and Moving Average Models (ARMA)

We might be interested in characterizing a time series in order to understand what happened in the past and to effectively forecast into the future. Two common models that can approximate time series are **autoregressive** and **moving average** models. To classify these models, we use the  **ACF (autocorrelation function)** and the **PACF (partial autocorrelation function)**, which correspond to the autocorrelation of a series and the correlation of the residuals, respectively. 


**Autoregressive** models operate under the framework that a given measurements is correlated with  previous measurements. For example, an AR1 formulation dictates that a measurement is dependent on the previous measurement, and the value can be predicted by quantifying the lag. 
- Any given measurement is correlated with previous measurement. or if on a cycle, correlations repeated

**Moving average** models operate under the framework that the covariance between a measurement and the previous measurement is zero. While AR models use past forecast *values* to predict future values, MA models use past forecast *errors* to predict future values.
- variance vs. covariance. use past forecast errors rather than values to predict future values

Here are some great resources for examining ACF and PACF lags under different formulations of AR and MA models. 
https://nwfsc-timeseries.github.io/atsa-labs/sec-tslab-autoregressive-ar-models.html
https://nwfsc-timeseries.github.io/atsa-labs/sec-tslab-moving-average-ma-models.html

ARMA models require stationary data. This means that there is no monotonic trend over time and there is also equal variance and covariance across the time series. The function `adf.test` will determine whether our data are stationary. The null hypothesis is that the data are not stationary, so we infer that the data are stationary if the p-value is < 0.05.
- no decreasing or increasing trend. 
-Can combined trend analysis to detrend data and format it to fit into trend analysis...

While some processes might be easy to identify, it is often complicated to predict the order of AR and MA processes when the operate in the same dataset. To get around this issue, it is often necessary to run multiple potential formulations of the model and see which one results in the most parsimonious fit using AIC. The function `auto.arima` does this automatically.

